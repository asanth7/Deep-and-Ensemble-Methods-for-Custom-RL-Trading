# Deep-and-Ensemble-Methods-for-Custom-RL-Trading
This repository includes ensemble methods and a deep Q-learning network for an RL trading agent acting in a custom OpenAI Gym environment.

The custom trading environment maintains the same Buy/Sell actions, Long/Short positions of the StocksEnv environment from OpenAI and still accounts for trading fees. However, this custom environment features modifications of the step() and _process_data() methods in addition to a host of other helper methods and environment methods necessary for the computation and processing of various metrics. The trading agent takes advantage of more than 20 economic indicators in addition to adjusted closing price data for any given stock in the S&P 500. The Advantage Actor Critic (A2C), Proximal Policy Optimization (PPO) and Deep Q-Network (DQN) models are used in an ensemble method. These models are analyzed independently and compared through custom callbacks that calculate the Sharpe and Calmar ratios.
