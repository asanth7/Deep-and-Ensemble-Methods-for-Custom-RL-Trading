# Deep-and-Ensemble-Methods-for-Custom-RL-Trading
This repository includes ensemble methods and a deep convolution and LSTM-based Q-learning network for an RL trading agent acting in a custom OpenAI Gym environment, utilizing more than 20 economic indicators and price data to make daily trading decisions.

The custom trading environment maintains the same Buy/Sell actions, Long/Short positions of the StocksEnv environment from OpenAI and still accounts for trading fees. However, this custom environment features modifications of the step() and _process_data() methods in addition to a host of other helper methods and environment methods necessary for the computation and processing of various metrics. The trading agent takes advantage of more than 20 economic indicators in addition to adjusted closing price data for any given stock in the S&P 500. The Advantage Actor Critic (A2C), Proximal Policy Optimization (PPO) and Deep Q-Network (DQN) models are used in an ensemble method. These models are analyzed independently and compared through custom callbacks that calculate the Sharpe and Calmar ratios.

The custom deep network achieved peak training profit of almost 60% (training data comprised of approximately 200 timesteps/days), and slightly over 7% in testing (over a 2 week period), which rivals the state-of-the-art A2C, PPO, and DQN models.
